{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbaaf8c",
   "metadata": {},
   "source": [
    "# MSMatch on EuroSAT RGB\n",
    "\n",
    "This notebook contains a condensed version of training a U-Net / EfficientNet-B0 on the EuroSAT RGB dataset using the semi-supervised MSMatch approach. For details on the method please refer to the paper. To run this notebook, please set up a conda environment with the provided environment.yml.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "os.chdir(\"c:\\\\Users\\\\Pablo Gomez\\\\OneDrive - ESA\\\\Documents\\\\Code\\\\Repos\\\\DistMSMatch\\\\\")\n",
    "\n",
    "# Main imports\n",
    "import torch\n",
    "import MSMatch as mm\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790889e0",
   "metadata": {},
   "source": [
    "The next variable contains the path to the configuration file `.toml`. If you set variable `cfg_path` was to `None`, the default configuration is used. Ohterwise, the cfg file speicified by the path is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaaff07",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "cfg_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a cfg DotMap (a dictionary with dot accessors) to store the configuration for the run\n",
    "cfg = mm.load_cfg(cfg_path)\n",
    "cfg.batch_size = 4\n",
    "cfg.uratio = 4\n",
    "cfg.ema = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e18f2a",
   "metadata": {},
   "source": [
    "Apply new configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f63108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility and enable loggers\n",
    "mm.set_seeds(cfg.seed)\n",
    "logger_level = \"INFO\"\n",
    "logger = mm.get_logger(cfg.save_path, logger_level)\n",
    "tb_log = mm.TensorBoardLog(cfg.save_path, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1150de",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning (SSL) Datasets\n",
    "\n",
    "In MSMatch we utilize both labeled and unlabeled data. To facilitiate this there is a class called `SSL_Dataset` in the MSMatch module which wraps around the `BasicDataset` class to take care of this.\n",
    "\n",
    "This is necessary as in each training iteration we provide a supervised and unsupervised loss term with different (stronger) augmentations for the unsupervised part. (see paper)\n",
    "\n",
    "We set up one dataset for training and one for testing.\n",
    "\n",
    "**Note**: For below cell to execute make sure you have placed the EuroSAT RGB dataset in a folder `data/EuroSAT_RGB` from the project root. (You should have folders like `AnnualCrop` etc inside above folder which contain the images for each class)\n",
    "\n",
    "You can download the dataset [from GitHub](https://madm.dfki.de/files/sentinel/EuroSAT.zip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00220e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Dataset\n",
    "print(\"Loading \" + colored(\"train\", \"red\") + \" dataset...\")\n",
    "train_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset,\n",
    "    train=True,\n",
    "    data_dir=None,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "lb_dset, ulb_dset = train_dset.get_ssl_dset(cfg.num_labels)\n",
    "\n",
    "cfg.num_classes = train_dset.num_classes\n",
    "cfg.num_channels = train_dset.num_channels\n",
    "\n",
    "print(\"Loading \" + colored(\"eval\", \"blue\") + \" dataset...\")\n",
    "_eval_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset,\n",
    "    train=False,\n",
    "    data_dir=None,\n",
    "    seed=cfg.seed,\n",
    ")\n",
    "eval_dset = _eval_dset.get_dset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b0be16",
   "metadata": {},
   "source": [
    "This project contains three different architectures for the backbone classifier, a very small U-Net-style encoder, the EfficientNet-Lite and the standard EfficientNet. Specify either `unet` or `efficientnet-b0` (up to `b7`) or `efficientnet-lite` in `cfg.net` to get the specific model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing \", cfg.net)\n",
    "net_builder = mm.get_net_builder(\n",
    "    cfg.net, pretrained=cfg.pretrained, in_channels=cfg.num_channels, scale=cfg.scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a67df",
   "metadata": {},
   "source": [
    "The main model we call `FixMatch`. It differs from pure backbone in sofar that it also implements features like the exponential moving average of the backbone weights and takes care of computing the different loss terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.FixMatch(\n",
    "    net_builder,\n",
    "    cfg.num_classes,\n",
    "    cfg.num_channels,\n",
    "    cfg.ema_m,\n",
    "    T=cfg.T,\n",
    "    p_cutoff=cfg.p_cutoff,\n",
    "    lambda_u=cfg.ulb_loss_ratio,\n",
    "    hard_label=True,\n",
    "    num_eval_iter=cfg.num_eval_iter,\n",
    "    tb_log=tb_log,\n",
    "    logger=logger,\n",
    ")\n",
    "logger.info(\n",
    "    f\"Number of Trainable Params: {sum(p.numel() for p in model.train_model.parameters() if p.requires_grad)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196fa8b0",
   "metadata": {},
   "source": [
    "Specify number of epochs to train, for convergence a value like 100 may be sensible. **Please, note:** the number of training epochs will not match `cfg.epoch` for `cfg.batch_size` different from 32. This is done to keep the number of images used during the whole training constant regardless of the batch size used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9114923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training iterations is based on that and how regularly we evaluate the model.\n",
    "# Note that batch size here only refers to the supervised part, so the real batch size\n",
    "# is cfg.batch_size * (1 + cfg.ulb_ratio)\n",
    "cfg.num_train_iter = cfg.epoch * cfg.num_eval_iter * 32 // cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer, ADAM and SGD are supported.\n",
    "optimizer = mm.get_optimizer(model.train_model, cfg.opt, cfg.lr, cfg.momentum, cfg.weight_decay)\n",
    "# We use a learning rate schedule to control the learning rate during training.\n",
    "scheduler = mm.get_cosine_schedule_with_warmup(\n",
    "    optimizer, cfg.num_train_iter, num_warmup_steps=cfg.num_train_iter * 0\n",
    ")\n",
    "model.set_optimizer(optimizer, scheduler)\n",
    "\n",
    "# If a CUDA capable GPU is used, we move everything to the GPU now\n",
    "if torch.cuda.is_available():\n",
    "    cfg.gpu = 0\n",
    "    torch.cuda.set_device(cfg.gpu)\n",
    "    model.train_model = model.train_model.cuda(cfg.gpu)\n",
    "    model.eval_model = model.eval_model.cuda(cfg.gpu)\n",
    "\n",
    "logger.info(f\"model_arch: {model}\")\n",
    "logger.info(f\"Arguments: {cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3c5fe",
   "metadata": {},
   "source": [
    "Originally the codebase supports parallel data augmentation and distributed learning on multiple GPUs in one machine. The below code is a bit of a relic of that. In practice it creates a generator to get a batch of augmented training images for labeled (lb) and unlabled (ulb) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb96272",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_dict = {}\n",
    "dset_dict = {\"train_lb\": lb_dset, \"train_ulb\": ulb_dset, \"eval\": eval_dset}\n",
    "\n",
    "loader_dict[\"train_lb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_lb\"],\n",
    "    cfg.batch_size,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=1,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"train_ulb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_ulb\"],\n",
    "    cfg.batch_size * cfg.uratio,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=1,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"eval\"] = mm.get_data_loader(dset_dict[\"eval\"], cfg.eval_batch_size, num_workers=1)\n",
    "\n",
    "## set DataLoader on FixMatch\n",
    "model.set_data_loader(loader_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2410052",
   "metadata": {},
   "source": [
    "Now we are ready to run the training! Abort as you see fit. This may spam the output a bit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = model.train\n",
    "trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a29026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "model.evaluate(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_run(\"latest_model.pth\", cfg.save_path, cfg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3.8.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "358bc857331bb613572dc0815176beebf9dc7b48105aaa7bacb21a2c5626dd13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
