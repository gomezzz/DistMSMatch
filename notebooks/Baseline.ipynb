{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543dc5f9",
   "metadata": {},
   "source": [
    "# MSMatch on EuroSAT RGB\n",
    "\n",
    "This notebook contains a condensed version of training a U-Net / EfficientNet-B0 on the EuroSAT RGB dataset using the semi-supervised MSMatch approach. For details on the method please refer to the paper. To run this notebook, please set up a conda environment with the provided environment.yml. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Main imports\n",
    "import torch\n",
    "import MSMatch as mm\n",
    "\n",
    "# We use a cfg DotMap (a dictionary with dot accessors) to store the configuration for the run\n",
    "cfg = mm.get_default_cfg()\n",
    "\n",
    "# Set seeds for reproducibility and enable loggers\n",
    "mm.set_seeds(cfg.seed)\n",
    "logger_level = \"INFO\"\n",
    "logger = mm.get_logger(cfg.save_name, cfg.save_path, logger_level)\n",
    "tb_log = mm.TensorBoardLog(cfg.save_path, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb258eb5",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning (SSL) Datasets \n",
    "In MSMatch we utilize both labeled and unlabeled data. To facilitiate this there is a class called `SSL_Dataset` in the MSMatch module which wraps around the `BasicDataset` class to take care of this.\n",
    "\n",
    "This is necessary as in each training iteration we provide a supervised and unsupervised loss term with different (stronger) augmentations for the unsupervised part. (see paper)\n",
    "\n",
    "We set up one dataset for training and one for testing.\n",
    "\n",
    "**Note**: For below cell to execute make sure you have placed the EuroSAT RGB dataset in a folder `data/EuroSAT_RGB` from the project root. (You should have folders like `AnnualCrop` etc inside above folder which contain the images for each class)\n",
    "\n",
    "You can download the dataset [from GitHub](https://madm.dfki.de/files/sentinel/EuroSAT.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9533969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Dataset\n",
    "train_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset, train=True, data_dir=None, seed=cfg.seed,\n",
    ")\n",
    "lb_dset, ulb_dset = train_dset.get_ssl_dset(cfg.num_labels)\n",
    "\n",
    "cfg.num_classes = train_dset.num_classes\n",
    "cfg.num_channels = train_dset.num_channels\n",
    "\n",
    "_eval_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset, train=False, data_dir=None, seed=cfg.seed,\n",
    ")\n",
    "eval_dset = _eval_dset.get_dset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de15da16",
   "metadata": {},
   "source": [
    "This project contains two different architectures for the backbone classifier, a very small U-Net-style encoder and the standard EfficientNet. Specify either `unet` or `efficientnet-b0` (up to `b7`) in `cfg.net` to get the specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing \", cfg.net)\n",
    "net_builder = mm.get_net_builder(\n",
    "    cfg.net,\n",
    "    pretrained=cfg.pretrained,\n",
    "    in_channels=cfg.num_channels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867aac2",
   "metadata": {},
   "source": [
    "The main model we call `FixMatch`. It differs from pure backbone in sofar that it also implements features like the exponential moving average of the backbone weights and takes care of computing the different loss terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee210e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.FixMatch(\n",
    "        net_builder,\n",
    "        cfg.num_classes,\n",
    "        cfg.num_channels,\n",
    "        cfg.ema_m,\n",
    "        T=0.5,\n",
    "        p_cutoff=cfg.p_cutoff,\n",
    "        lambda_u=cfg.ulb_loss_ratio,\n",
    "        hard_label=True,\n",
    "        num_eval_iter=cfg.num_eval_iter,\n",
    "        tb_log=tb_log,\n",
    "        logger=logger,\n",
    "    )\n",
    "logger.info(f\"Number of Trainable Params: {sum(p.numel() for p in model.train_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1180564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of epochs to train, for convergence a value like 100 may be sensible\n",
    "cfg.epoch = 10\n",
    "\n",
    "# Number of training iterations is based on that and how regularly we evaluate the model.\n",
    "# Note that batch size here only refers to the supervised part, so the real batch size\n",
    "# is cfg.batch_size * (1 + cfg.ulb_ratio)\n",
    "cfg.num_train_iter = cfg.epoch * cfg.num_eval_iter * 32 // cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7b1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer, ADAM and SGD are supported.\n",
    "optimizer = mm.get_optimizer(\n",
    "    model.train_model, cfg.opt, cfg.lr, cfg.momentum, cfg.weight_decay\n",
    ")\n",
    "# We use a learning rate schedule to control the learning rate during training.\n",
    "scheduler = mm.get_cosine_schedule_with_warmup(\n",
    "    optimizer, cfg.num_train_iter, num_warmup_steps=cfg.num_train_iter * 0\n",
    ")\n",
    "model.set_optimizer(optimizer, scheduler)\n",
    "\n",
    "# If a CUDA capable GPU is used, we move everything to the GPU now\n",
    "if torch.cuda.is_available():\n",
    "    cfg.gpu = 0\n",
    "    torch.cuda.set_device(cfg.gpu)\n",
    "    model.train_model = model.train_model.cuda(cfg.gpu)\n",
    "    model.eval_model = model.eval_model.cuda(cfg.gpu)\n",
    "\n",
    "logger.info(f\"model_arch: {model}\")\n",
    "logger.info(f\"Arguments: {cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef07dbd2",
   "metadata": {},
   "source": [
    "Originally the codebase supports parallel data augmentation and distributed learning on multiple GPUs in one machine. The below code is a bit of a relic of that. In practice it takes of creating a generator to get a batch of augmented training images for labeled (lb) and unlabled (ulb) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e096708",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_dict = {}\n",
    "dset_dict = {\"train_lb\": lb_dset, \"train_ulb\": ulb_dset, \"eval\": eval_dset}\n",
    "\n",
    "loader_dict[\"train_lb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_lb\"],\n",
    "    cfg.batch_size,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=1,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"train_ulb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_ulb\"],\n",
    "    cfg.batch_size * cfg.uratio,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=4,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"eval\"] = mm.get_data_loader(\n",
    "    dset_dict[\"eval\"], cfg.eval_batch_size, num_workers=1\n",
    ")\n",
    "\n",
    "## set DataLoader on FixMatch\n",
    "model.set_data_loader(loader_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4caa9b",
   "metadata": {},
   "source": [
    "Now we are ready to run the training! Abort as you see fit. This may spam the output a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = model.train\n",
    "print(cfg)\n",
    "\n",
    "for epoch in range(cfg.epoch):\n",
    "    print(epoch)\n",
    "    trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7aec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "model.evaluate(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"latest_model.pth\", cfg.save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torchmatch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddf264bb344b85a2f0436b8e15459b084b5ee9678e571d1be85bf8d829f31722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
