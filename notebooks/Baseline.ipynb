{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3031d762",
   "metadata": {},
   "source": [
    "# MSMatch on EuroSAT RGB\n",
    "\n",
    "This notebook contains a condensed version of training a U-Net / EfficientNet-B0 on the EuroSAT RGB dataset using the semi-supervised MSMatch approach. For details on the method please refer to the paper. To run this notebook, please set up a conda environment with the provided environment.yml. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71ca912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Utility imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Main imports\n",
    "import torch\n",
    "import MSMatch as mm\n",
    "from MSMatch.utils.create_dir_string import create_dir_str\n",
    "from MSMatch.utils.load_cfg import load_cfg\n",
    "from termcolor import colored\n",
    "from  MSMatch.utils.print_cfg import print_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d63bbe",
   "metadata": {},
   "source": [
    "The next variable contains the path to the configuration file `.toml`.  If you set variable `cfg_path` was to `None`, the default configuration is used. Ohterwise, the cfg file speicified by the path is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76baf7c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "cfg_path=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fc73c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standard configuration...\n",
      "dataset                     : eurosat_rgb    |net                         : unet           |batch_size                  : 32             |\n",
      "p_cutoff                    : 0.95           |lr                          : 0.01           |uratio                      : 7              |\n",
      "weight_decay                : 0.00075        |ulb_loss_ratio              : 1.0            |seed                        : 42             |\n",
      "num_labels                  : 100            |opt                         : SGD            |pretrained                  : 0              |\n",
      "save_dir                    : ./saved_models |save_name_root              : test           |ema_m                       : 0.999          |\n",
      "bn_momentum                 : 0.0010000000000000009|eval_batch_size             : 1024           |momentum                    : 0.9            |\n",
      "T                           : 0.5            |amp                         : 0              |hard_label                  : 1              |\n",
      "multiprocessing_distributed: 0              |num_eval_iter               : 1000           |scale                       : 1              |\n",
      "save_name                   : test/eurosat_rgb/FixMatch_archunet_batch32_confidence0.95_lr0.01_uratio7_wd0.00075_wu1.0_seed42_numlabels100_optSGD|save_path                   : ./saved_models/test/eurosat_rgb/FixMatch_archunet_batch32_confidence0.95_lr0.01_uratio7_wd0.00075_wu1.0_seed42_numlabels100_optSGD|\n"
     ]
    }
   ],
   "source": [
    "# We use a cfg DotMap (a dictionary with dot accessors) to store the configuration for the run\n",
    "if cfg_path is not None:\n",
    "    print(\"Using configuration file: \"+colored(cfg_path,\"red\"))\n",
    "    cfg=load_cfg(cfg_path)\n",
    "else:\n",
    "    print(\"Using standard configuration...\")\n",
    "    cfg = mm.get_default_cfg()\n",
    "\n",
    "print_cfg(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4500bb",
   "metadata": {},
   "source": [
    "Apply new configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eed5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility and enable loggers\n",
    "mm.set_seeds(cfg.seed)\n",
    "logger_level = \"INFO\"\n",
    "logger = mm.get_logger(cfg.save_name, cfg.save_path, logger_level)\n",
    "tb_log = mm.TensorBoardLog(cfg.save_path, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7200580d",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning (SSL) Datasets \n",
    "In MSMatch we utilize both labeled and unlabeled data. To facilitiate this there is a class called `SSL_Dataset` in the MSMatch module which wraps around the `BasicDataset` class to take care of this.\n",
    "\n",
    "This is necessary as in each training iteration we provide a supervised and unsupervised loss term with different (stronger) augmentations for the unsupervised part. (see paper)\n",
    "\n",
    "We set up one dataset for training and one for testing.\n",
    "\n",
    "**Note**: For below cell to execute make sure you have placed the EuroSAT RGB dataset in a folder `data/EuroSAT_RGB` from the project root. (You should have folders like `AnnualCrop` etc inside above folder which contain the images for each class)\n",
    "\n",
    "You can download the dataset [from GitHub](https://madm.dfki.de/files/sentinel/EuroSAT.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859a224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Dataset\n",
    "print(\"Loading \"+colored(\"train\", \"red\")+ \" dataset...\")\n",
    "train_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset, train=True, data_dir=None, seed=cfg.seed,\n",
    ")\n",
    "lb_dset, ulb_dset = train_dset.get_ssl_dset(cfg.num_labels)\n",
    "\n",
    "cfg.num_classes = train_dset.num_classes\n",
    "cfg.num_channels = train_dset.num_channels\n",
    "\n",
    "print(\"Loading \"+colored(\"eval\", \"blue\")+ \" dataset...\")\n",
    "_eval_dset = mm.SSL_Dataset(\n",
    "    name=cfg.dataset, train=False, data_dir=None, seed=cfg.seed,\n",
    ")\n",
    "eval_dset = _eval_dset.get_dset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa52cb",
   "metadata": {},
   "source": [
    "This project contains two different architectures for the backbone classifier, a very small U-Net-style encoder and the standard EfficientNet. Specify either `unet` or `efficientnet-b0` (up to `b7`) in `cfg.net` to get the specific model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed62024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing \", cfg.net)\n",
    "net_builder = mm.get_net_builder(\n",
    "    cfg.net,\n",
    "    pretrained=cfg.pretrained,\n",
    "    in_channels=cfg.num_channels,\n",
    "    scale=cfg.scale\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fab783",
   "metadata": {},
   "source": [
    "The main model we call `FixMatch`. It differs from pure backbone in sofar that it also implements features like the exponential moving average of the backbone weights and takes care of computing the different loss terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee5ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mm.FixMatch(\n",
    "        net_builder,\n",
    "        cfg.num_classes,\n",
    "        cfg.num_channels,\n",
    "        cfg.ema_m,\n",
    "        T=cfg.T,\n",
    "        p_cutoff=cfg.p_cutoff,\n",
    "        lambda_u=cfg.ulb_loss_ratio,\n",
    "        hard_label=True,\n",
    "        num_eval_iter=cfg.num_eval_iter,\n",
    "        tb_log=tb_log,\n",
    "        logger=logger,\n",
    "    )\n",
    "logger.info(f\"Number of Trainable Params: {sum(p.numel() for p in model.train_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e507072",
   "metadata": {},
   "source": [
    "Specify number of epochs to train, for convergence a value like 100 may be sensible. **Please, note:** the number of training epochs will not match `cfg.epoch` for `cfg.batch_size` different from 32. This is done to keep the number of images used during the whole training constant regardless of the batch size used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a70de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training iterations is based on that and how regularly we evaluate the model.\n",
    "# Note that batch size here only refers to the supervised part, so the real batch size\n",
    "# is cfg.batch_size * (1 + cfg.ulb_ratio)\n",
    "cfg.num_train_iter = cfg.epoch * cfg.num_eval_iter * 32 // cfg.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92771acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer, ADAM and SGD are supported.\n",
    "optimizer = mm.get_optimizer(\n",
    "    model.train_model, cfg.opt, cfg.lr, cfg.momentum, cfg.weight_decay\n",
    ")\n",
    "# We use a learning rate schedule to control the learning rate during training.\n",
    "scheduler = mm.get_cosine_schedule_with_warmup(\n",
    "    optimizer, cfg.num_train_iter, num_warmup_steps=cfg.num_train_iter * 0\n",
    ")\n",
    "model.set_optimizer(optimizer, scheduler)\n",
    "\n",
    "# If a CUDA capable GPU is used, we move everything to the GPU now\n",
    "if torch.cuda.is_available():\n",
    "    cfg.gpu = 0\n",
    "    torch.cuda.set_device(cfg.gpu)\n",
    "    model.train_model = model.train_model.cuda(cfg.gpu)\n",
    "    model.eval_model = model.eval_model.cuda(cfg.gpu)\n",
    "\n",
    "logger.info(f\"model_arch: {model}\")\n",
    "logger.info(f\"Arguments: {cfg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f21f8f1",
   "metadata": {},
   "source": [
    "Originally the codebase supports parallel data augmentation and distributed learning on multiple GPUs in one machine. The below code is a bit of a relic of that. In practice it takes of creating a generator to get a batch of augmented training images for labeled (lb) and unlabled (ulb) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_dict = {}\n",
    "dset_dict = {\"train_lb\": lb_dset, \"train_ulb\": ulb_dset, \"eval\": eval_dset}\n",
    "\n",
    "loader_dict[\"train_lb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_lb\"],\n",
    "    cfg.batch_size,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=1,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"train_ulb\"] = mm.get_data_loader(\n",
    "    dset_dict[\"train_ulb\"],\n",
    "    cfg.batch_size * cfg.uratio,\n",
    "    data_sampler=\"RandomSampler\",\n",
    "    num_iters=cfg.num_train_iter,\n",
    "    num_workers=4,\n",
    "    distributed=False,\n",
    ")\n",
    "\n",
    "loader_dict[\"eval\"] = mm.get_data_loader(\n",
    "    dset_dict[\"eval\"], cfg.eval_batch_size, num_workers=1\n",
    ")\n",
    "\n",
    "## set DataLoader on FixMatch\n",
    "model.set_data_loader(loader_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f49d75",
   "metadata": {},
   "source": [
    "Now we are ready to run the training! Abort as you see fit. This may spam the output a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39637aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = model.train\n",
    "trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "model.evaluate(cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7faf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"latest_model.pth\", cfg.save_path)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python (distmsmatch)",
   "language": "python",
   "name": "distmsmatch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ddf264bb344b85a2f0436b8e15459b084b5ee9678e571d1be85bf8d829f31722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
